<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaosong Jia</title>
  
  <meta name="author" content="Xiaosong Jia">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="u8S0zPtl4mCVVwZ0YUFtVrgQnq9eBhl9N4W83xsN0xw" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaosong Jia (贾萧松)</name><br />
		<font size="2"> jiaxiaosong1997 [At] gmail [dot] com <font size="2">
              </p>
              <p>I am a second year CS PhD student at Shanghai Jiao Tong University, adviced by Prof. <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>. .
              </p>
              <p>
                 Previously, I received my B.Eng in Computer Science from Zhiyuan Honor Class, Shanghai Jiao Tong University. I have the fortune to work with (chronological order): Prof. <a href="https://www.cs.sjtu.edu.cn/~gao-xf/"> Xiaofeng Gao</a> at SJTU, 
		      Dr. <a href="https://zheng-da.github.io/">Da Zheng</a> and Prof. <a href="https://shanghai.nyu.edu/academics/faculty/directory/zheng-zhang">Zheng Zhang</a> at Amazon,
		      Dr. <a href="https://zhanwei.site/"> Wei Zhan </a>, Dr. <a href="https://scholar.google.com/citations?user=BitIg-YAAAAJ&hl=en"> Liting Sun </a>, and Prof. <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a> at UC Berkeley,
		      Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> at Tsinghua University, Dr. <a href="https://lihongyang.info/">Hongyang Li</a> at Shanghai AI Lab.
              </p>
              <p style="text-align:center">
                <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp
                <!--<a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=JeFQwxUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jiaxiaosong1002/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xiaosongjia.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xiaosongjia.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
		 I am interestied in machine learning and autonomous driving. Currently, I am focusing on the <b>trajectory prediction, reinforcement learning, and end-to-end autonomous driving.
              </p>
            </td>
          </tr>
	
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publication & Project</heading>
            </td>
          </tr>
	</tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/driveadapter.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2308.00398" id="MCG_journal">
                <papertitle>DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving</papertitle>
              </a>
              <br>
		    <b>Xiaosong Jia</b>, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, Hongyang Li
              <br>
              <b>ICCV</b>, 2023 <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <p></p> 
              <p> New paradigm for end-to-end autonomous driving without causal confusion. </p>
            </td>
          </tr>
        </tbody></table>
	
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/thinktwice.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.09753" id="MCG_journal">
                <papertitle>Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</papertitle>
              </a>
              <br>
		    <b>Xiaosong Jia</b>, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, Hongyang Li
              <br>
              <b>CVPR</b>, 2023
              <br>
              <p></p> 
              <p> BEV-based scalable end-to-end autonomous drving model. </p>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uniad.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2212.10156" id="MCG_journal">
                <papertitle>Planning-oriented Autonomous Driving</papertitle>
              </a>
              <br>
		    Yihan Hu*, Jiazhi Yang*, Li Chen*, Keyu Li*, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, <b>Xiaosong Jia</b>, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li
              <br>
              <b>CVPR</b>, 2023 <font color="red"><strong>(Best Paper Award)</strong></font>
              <br>
              <p></p> 
              <p> All modules in one Transformer-based end-to-end network for autonomous driving. </p>
            </td>
          </tr>
        </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hdgt.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.09753" id="MCG_journal">
                <papertitle>HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding</papertitle>
              </a>
              <br>
		    <b> Xiaosong Jia </b>, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan
              <br>
              <b>TPAMI</b>, 2023
              <br>
              <p></p> 
              <p> Unified heterogeneous graph neural network for driving scene encoding. SOTA methods on INTERACTION and Waymo challenge. </p>
            </td>
          </tr>
        </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ppgeo.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2301.01006" id="MCG_journal">
                <papertitle> PPGeo: Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling </papertitle>
              </a>
              <br>
		    Penghao Wu, Li Chen, Hongyang Li,  <b>Xiaosong Jia</b>, Junchi Yan, Yu Qiao
              <br>
             <b>ICLR</b>, 2023
              <br>
              <p></p> 
              <p>  Self-supervised pretraining for policy learning </p>
            </td>
         </tr>
	</tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tcp.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2206.08129" id="MCG_journal">
                <papertitle> TCP: Trajectory-guided Control Prediction for Autonomous Driving </papertitle>
              </a>
              <br>
		    Penghao Wu*, <b> Xiaosong Jia* </b>, Li Chen*, Junchi Yan, Hongyang Li, Yu Qiao
              <br>
             <b>NeurIPS</b>, 2022
              <br>
              <p></p> 
              <p>  Trajectory-guided control paradigm for end-to-end autonomous driving. 1st method on Carla Leaderboard, with only a monocular camera, outperforming other methods with multiple cameras and LiDAR by a large margin. </p>
            </td>
         </tr>
	</tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/temporal.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=PZiKO7mjC43" id="MCG_journal">
                <papertitle>Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach</papertitle>
              </a>
              <br>
		    <b> Xiaosong Jia </b>, Li Chen, Penghao Wu, Jia Zeng, Junchi Yan, Hongyang Li, Yu Qiao
              <br>
              <b>CoRL</b>, 2022
              <br>
              <p></p> 
              <p> A plug-and-play module for trajectory prediction by enhancing the temporal correlation among the predicted time-steps. </p>
            </td>
          </tr>
	</tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/corl2021.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=lAtePxetBNb" id="MCG_journal">
                <papertitle>Multi-Agent Trajectory Prediction by Combining Egocentric and Allocentric Views </papertitle>
              </a>
              <br>
		    <b> Xiaosong Jia </b>, Liting Sun, Hang Zhao, Masayoshi Tomizuka, Wei Zhan
              <br>
              <b>CoRL</b>, 2021 <br />
	      <b>ICCV Mair2 Workshop</b>, 2021 <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <p></p> 
              <p> Rethink the invariance property of the coordinate reprentation for trajectory prediction. </p>
            </td>
          </tr>
	</tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/interpret.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://challenge.interaction-dataset.com/prediction-challenge/intro" id="MCG_journal">
                <papertitle>INTERPRET: INTERACTION-Dataset-Based PREdicTion Challenge </papertitle>
              </a>
              <br>
		    Wei Zhan, Liting Sun, Hengbo Ma, Chenran Li, <b>Xiaosong Jia</b>, Masayoshi Tomizuka
              <br>
              <p></p> 
              <p> Co-organized the competition in ICCV 2021. I was responsible for the design and implementation of the Joint Prediction and Conditional Prediction Tracks. </p>
            </td>
          </tr>
	</tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ral2021.PNG" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2011.02403.pdf" id="MCG_journal">
                <papertitle>IDE-Net: Interactive Driving Event and Pattern Extraction from Human Data </papertitle>
              </a>
              <br>
		    <b> Xiaosong Jia </b>, Liting Sun, Masayoshi Tomizuka, Wei Zhan
              <br>
              <b>RA Letters</b>, 2021 <br />
	      <b>ICRA</b>, 2021
              <br>
              <p></p> 
              <p> Unsupervisedly extracting interactive behaviors in a <em> whether, when, and what <em> hierarchy.</p>
            </td>
          </tr>
	</tbody></table>
	
	
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a> is from Jon Barron
              </p>
            </td>
          </tr>
        </tbody></table>
		
      </td>
    </tr>
  </table>
</body>

</html>
